<!DOCTYPE html>
<!--
                                   _       _   
								  | |     | |    
  Designed and Coded By           | | __ _| |__  
 / _ \ \ /\ / /\ \ /\ / /\ \ /\ / / |/ _` | '_ \ 
| (_) \ V  V /  \ V  V /  \ V  V /| | (_| | |_) |
 \___/ \_/\_/    \_/\_/    \_/\_/ |_|\__,_|_.__/ 

http://themeforest.net/user/owwwlab/
                                          
-->
<html lang="en" dir="ltr" itemscope itemtype="http://schema.org/WebPage">
  <!-- #####Begin head-->
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- globalise scripting and styling content language-->
    <meta name="Content-Type-Script" content="text/javascript">
    <meta name="Content-Type-Style" content="text/css">
    <!-- author of this doc-->
    <meta name="author" content="owwwlab.com">
    <!-- ################################-->
    <!-- #### add SEO metadata here-->
    <!-- ################################-->
    <!-- #####Begin styles-->
    <!-- stylesheets	-->
    <link href="assets/css/vendors/vendors.css" rel="stylesheet" type="text/css">
    <!-- Load extra page specific css-->
    <!-- Overwrite vendors-->
    <link href="assets/css/vendors/vendors-overwrites.css" rel="stylesheet" type="text/css">
    <link href="assets/css/styles.css" rel="stylesheet" type="text/css">
    <link href="assets/css/demo2.css" rel="stylesheet" type="text/css">
    <!-- #####End styles-->
    <!-- #####Begin JS-->
    <!-- add your scripts to the end of the page for sake of page load speed-->
    <!-- scripts that need to be at head section-->
    <script src="assets/js/vendors/jquery.min.js"></script>
    <!-- #####End JS-->
    <!-- #####Begin load google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Play:400,700&amp;subset=latin,greek,cyrillic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Sintony:400,700&amp;subset=latin,greek,cyrillic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Merriweather:300,300italic,400,400italic,700,700italic&amp;subset=latin,greek,cyrillic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Exo+2:400,200,300,400italic,500,700,900&amp;subset=latin,greek,cyrillic" rel="stylesheet" type="text/css">
    <!-- #####End load google fonts-->
    <title>Publications - CHAOS HTML Template</title>
  </head>
  <!-- #####End head-->
  <body class="fullwidth bg-gray sticky-header">
    <div id="wrapper" class="boxed-layout-padding">
      <header id="header" data-sticky-threshold='-300' class=" light sticky-light">
        <div class="head-main slim-container text-center">
          <div class="logo-wrapper">
            <!-- standard Logo--><a href="index.html" class="standard-logo text-logo"><img src="assets/img/avatar.jpg" alt="Dr. Chaos Mctabi">
              <div class="text-wrapper">
                <h1 class="title">Hieu Nguyen</h1>
              </div></a>
          </div>
          <!-- primary navigation-->
          <ul id="primary-menu">
            <li><a href="index.html" title="Home"><span>Home</span></a>
            </li>
            <li class="current-menu-item"><a href="publications.html" title="Publications"><span>Publications</span></a>
            </li>
            <li><a href="blog.html" title="Blog"><span>Blog</span></a>
            </li>
            <li><a href="gallery.html" title="Gallery"><span>Gallery</span></a>
            </li>
            <li><a href="contact.html" title="Contact"><span>Contact</span></a>
            </li>
          </ul>
          <!-- #####Begin icons beside menu-->
          <div class="header-icons">
            <!-- #####Begin mobile menu trigger-->
            <div class="ol-mobile-trigger hamburger hamburger--elastic">
              <div class="hamburger-box">
                <div class="hamburger-inner"></div>
              </div>
            </div>
            <!-- #####End mobile menu trigger-->
          </div>
        </div>
      </header>
      <!-- end #header-->
      <section id="contents">
        <!-- handle parallax mods-->
        <div class="page-head h-400 center-it parallax-layer dark" data-img-src="assets/img/bg/06-2.jpg" data-parallax-mode="mode-title">
          <div class="container">
            <div class="tb-vcenter-wrapper">
              <div class="title-wrapper vcenter parallax-layer" data-parallax-mode="mode-header-demo-2">
                <h4 class="head">I HAVE DONE</h4>
                <h1 class="title">Publications</h1>
              </div>
            </div>
          </div>
        </div>
        <div class="page-contents">
          <!-- #####Begin main area-->
          <section id="main-area">
            <section class="slim-container">
              <div class="container">



<div class="row">
	<div class="col-md-3"><a href="http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html"> <img src="http://cvlab.cse.msu.edu/images/teasers/nonlinear-3dmm.png"></a></div>
	<div class="col-md-9">
		<h4 class="m-bottom-30"> <a href="http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html">Nonlinear 3D Face Morphable Model</a></h4>
		<div class="dl-horizontal text-left tight m-bottom-30">
		<div class="details">As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, including model fitting, image synthesis, etc. Conventional 3DMM is learned from a collection of wellcontrolled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two network decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analyticallydifferentiable rendering layer is designed to reconstruct the original input face. The entire architecture is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D face reconstruction.</div>		</div>
	</div>
</div>
<div class="sp-line-80"></div>
<div class="row">
	<div class="col-md-3"><a href="http://cvlab.cse.msu.edu/project-dr-gan.html"> <img src="http://cvlab.cse.msu.edu/images/teasers/dr-gan.jpg"></a></div>
	<div class="col-md-9">
		<h4 class="m-bottom-30"> <a href="http://cvlab.cse.msu.edu/project-dr-gan.html">Disentangled Representation Learning GAN for Pose-Invariant Face Recognition</a></h4>
		<div class="dl-horizontal text-left tight m-bottom-30">
		<div class="details">The large pose discrepancy between two face images is one of the key challenges in face recognition. The conventional approach to pose-robust face recognition either performs face frontalization on, or learns a pose-invariant representation from, a non-frontal face image. We argue that, it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation Learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn the identity representation for each face image, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one integrated representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both constrained and unconstrained databases demonstrate the superiority of DR-GAN over the state of the art.</div>		</div>
	</div>
</div>
<div class="sp-line-80"></div>
<div class="row">
	<div class="col-md-3"><a href="http://cvlab.cse.msu.edu/project-fuse-missing-modalities.html"> <img src="http://cvlab.cse.msu.edu/images/teasers/fuse-missing-modalities.png"></a></div>
	<div class="col-md-9">
		<h4 class="m-bottom-30"> <a href="http://cvlab.cse.msu.edu/project-fuse-missing-modalities.html">Learning to Fuse Information with Missing Modalities</a></h4>
		<div class="dl-horizontal text-left tight m-bottom-30">
		<div class="details">One of the key GEOINT capabilities is to be able to automatically recognize a large array of objects from visual data. Depending on the resolution of imagery, objects may range from specific locations or scenes, road, building, forest to vehicle, human, etc. This is clearly a technically challenging problem for both computer vision and machine learning due to the large variations in the appearance of these objects exhibited in the imagery. To address this problem, researchers have developed various fusion methods that combine information collected from multiple sensing modalities, such as RGB imagery, LiDAR point cloud, multispectral imaging, hyperspectral imaging, and GPS, to improve the reliability and accuracy of object recognition. This research direction is motivated by the ever-decreasing sensoring cost, and more importantly, by the complementary characteristics among multiple sensing modalities. Therefore, with the well-funded promise of escalating object recognition performance, a great deal of data analysis research is in urgent need in order to fully take advantage of this massive amount of multi-modality data.<br />
<br />
All the prior research oninformation fusion requires that the sensor data of all modalities are available for every training data instance. This requirement significantly limits the application of information fusion methods as missing modalities abound in practical applications.<br />
<br />
In recognizing missing modalities as a roadblock toward fulfilling the key GEOINT capability, we propose to develop powerful and computationally efficient approaches that can learn to fuse information from different sensors when a significant portion of training data has missing modalities. The ultimate goal of our project is to develop a suite of computer vision and machine learning tools for geographical imagery analysis that can serve as an aid for geo-spatial analysts to facilitate the analysis and classification of geographical images.</div>		</div>
	</div>
</div>
<div class="sp-line-80"></div>
<div class="row">
	<div class="col-md-3"><a href="#"> <img src="http://www.egr.msu.edu/3dvision/Images/HumanOfficeSkeleton.png"></a></div>
	<div class="col-md-9">
		<h4 class="m-bottom-30"> <a href="#">Person tracking and motion analysis for medical applications</a></h4>
		<div class="dl-horizontal text-left tight m-bottom-30">
		<div class="details">New RGBD sensors enable precise tracking of human motions. This can be used for medical appications such as home care.</div>		</div>
	</div>
</div>
<div class="sp-line-80"></div>


              </div>
            </section>
          </section>
          <!-- #####End main area
          <div class="clearfix"></div>
        </div>
      </section>
      <!-- end #contents-->
      <div class="row slim-container bg-white">
        <div class="container">
          <div class="sp-line-0 bg-white"></div>
        </div>
      </div>
      <footer id="footer">
        <div id="footer-main" class="text-center bg-white">
          <div class="row">
            <div class="container">
              <div class="col-md-6 col-md-offset-3">
                <ul class="social-icons shape-circle hover-theme m-top-20">
                  <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                  <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                  <li><a href="#"><i class="fa fa-google-plus"></i></a></li>
                  <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                  <li><a href="#"><i class="fa fa-skype"></i></a></li>
                  <li><a href="#"><i class="fa fa-dribbble"></i></a></li>
                </ul>
                <div class="sp-blank-20"></div>
                <div class="copyright">owwwlab 2015</div>
              </div>
            </div>
          </div>
        </div>
      </footer>
      <!-- end #footer-->
    </div>
    <!-- #####Begin scripts-->
    <script src="assets/js/vendors/vendors.js"></script>
    <!-- Local Revolution tools-->
    <!-- Only for local and can be removed on server-->
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.video.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.slideanims.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.actions.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.layeranimation.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.kenburn.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.navigation.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.migration.min.js"></script>
    <script type="text/javascript" src="assets/revolution/js/extensions/revolution.extension.parallax.min.js"></script>
    <script src="assets/js/custom.js"></script>
    <script src="http://localhost:35729/livereload.js"></script>
    <!-- #####End scripts-->
  </body>
</html>